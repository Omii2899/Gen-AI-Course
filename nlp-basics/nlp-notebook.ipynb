{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/omii/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/omii/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. \n",
    "It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. \\nIt has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "### Paragraphs --> Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nLorem Ipsum is simply dummy text of the printing and typesetting industry.',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\",\n",
       " 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.',\n",
       " 'It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "### Paragraph --> Words\n",
    "### Sentence --> Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem',\n",
       " 'Ipsum',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'of',\n",
       " 'the',\n",
       " 'printing',\n",
       " 'and',\n",
       " 'typesetting',\n",
       " 'industry',\n",
       " '.',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'has',\n",
       " 'been',\n",
       " 'the',\n",
       " 'industry',\n",
       " \"'s\",\n",
       " 'standard',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'the',\n",
       " '1500s',\n",
       " ',',\n",
       " 'when',\n",
       " 'an',\n",
       " 'unknown',\n",
       " 'printer',\n",
       " 'took',\n",
       " 'a',\n",
       " 'galley',\n",
       " 'of',\n",
       " 'type',\n",
       " 'and',\n",
       " 'scrambled',\n",
       " 'it',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'type',\n",
       " 'specimen',\n",
       " 'book',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'survived',\n",
       " 'not',\n",
       " 'only',\n",
       " 'five',\n",
       " 'centuries',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'the',\n",
       " 'leap',\n",
       " 'into',\n",
       " 'electronic',\n",
       " 'typesetting',\n",
       " ',',\n",
       " 'remaining',\n",
       " 'essentially',\n",
       " 'unchanged',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'popularised',\n",
       " 'in',\n",
       " 'the',\n",
       " '1960s',\n",
       " 'with',\n",
       " 'the',\n",
       " 'release',\n",
       " 'of',\n",
       " 'Letraset',\n",
       " 'sheets',\n",
       " 'containing',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'passages',\n",
       " ',',\n",
       " 'and',\n",
       " 'more',\n",
       " 'recently',\n",
       " 'with',\n",
       " 'desktop',\n",
       " 'publishing',\n",
       " 'software',\n",
       " 'like',\n",
       " 'Aldus',\n",
       " 'PageMaker',\n",
       " 'including',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem',\n",
       " 'Ipsum',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'of',\n",
       " 'the',\n",
       " 'printing',\n",
       " 'and',\n",
       " 'typesetting',\n",
       " 'industry',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem',\n",
       " 'Ipsum',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'of',\n",
       " 'the',\n",
       " 'printing',\n",
       " 'and',\n",
       " 'typesetting',\n",
       " 'industry',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem',\n",
       " 'Ipsum',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'of',\n",
       " 'the',\n",
       " 'printing',\n",
       " 'and',\n",
       " 'typesetting',\n",
       " 'industry.',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'has',\n",
       " 'been',\n",
       " 'the',\n",
       " 'industry',\n",
       " \"'s\",\n",
       " 'standard',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'the',\n",
       " '1500s',\n",
       " ',',\n",
       " 'when',\n",
       " 'an',\n",
       " 'unknown',\n",
       " 'printer',\n",
       " 'took',\n",
       " 'a',\n",
       " 'galley',\n",
       " 'of',\n",
       " 'type',\n",
       " 'and',\n",
       " 'scrambled',\n",
       " 'it',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'type',\n",
       " 'specimen',\n",
       " 'book.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'survived',\n",
       " 'not',\n",
       " 'only',\n",
       " 'five',\n",
       " 'centuries',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'the',\n",
       " 'leap',\n",
       " 'into',\n",
       " 'electronic',\n",
       " 'typesetting',\n",
       " ',',\n",
       " 'remaining',\n",
       " 'essentially',\n",
       " 'unchanged.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'popularised',\n",
       " 'in',\n",
       " 'the',\n",
       " '1960s',\n",
       " 'with',\n",
       " 'the',\n",
       " 'release',\n",
       " 'of',\n",
       " 'Letraset',\n",
       " 'sheets',\n",
       " 'containing',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'passages',\n",
       " ',',\n",
       " 'and',\n",
       " 'more',\n",
       " 'recently',\n",
       " 'with',\n",
       " 'desktop',\n",
       " 'publishing',\n",
       " 'software',\n",
       " 'like',\n",
       " 'Aldus',\n",
       " 'PageMaker',\n",
       " 'including',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\", \"eat\", \"writes\", \"written\", \"wrote\", \"finally\", \"final\", \"dolphin\", \"writing\", \"history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eat----->eat\n",
      "writes----->write\n",
      "written----->written\n",
      "wrote----->wrote\n",
      "finally----->final\n",
      "final----->final\n",
      "dolphin----->dolphin\n",
      "writing----->write\n",
      "history----->histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"----->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Regex Stemmer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'poch'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('poche')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eat---->eat\n",
      "writes---->write\n",
      "written---->written\n",
      "wrote---->wrote\n",
      "finally---->final\n",
      "final---->final\n",
      "dolphin---->dolphin\n",
      "writing---->write\n",
      "history---->histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word}---->{stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmetization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.WordnetLemmetizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmetizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS \n",
    "- verb - v\n",
    "- Noun - n(default)\n",
    "- adjective - a\n",
    "- adverb - r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmetizer.lemmatize(\"going\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmetizer.lemmatize(\"going\", pos=\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eat---->eat\n",
      "writes---->write\n",
      "written---->write\n",
      "wrote---->write\n",
      "finally---->finally\n",
      "final---->final\n",
      "dolphin---->dolphin\n",
      "writing---->write\n",
      "history---->history\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    x = lemmetizer.lemmatize(word, pos=\"v\")\n",
    "    print(f\"{word}---->{x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = \"\"\"\n",
    "I am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation.\n",
    "Five score years ago, a great American, in whose symbolic shadow we stand today, signed the Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions of Negro slaves who had been seared in the flames of withering injustice. It came as a joyous daybreak to end the long night of their captivity.\n",
    "But one hundred years later, the Negro still is not free. One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination. One hundred years later, the Negro lives on a lonely island of poverty in the midst of a vast ocean of material prosperity. One hundred years later, the Negro is still languished in the corners of American society and finds himself an exile in his own land. And so we've come here today to dramatize a shameful condition.\n",
    "In a sense we've come to our nation's capital to cash a check. When the architects of our republic wrote the magnificent words of the Constitution and the Declaration of Independence, they were signing a promissory note to which every American was to fall heir. This note was a promise that all men, yes, black men as well as white men, would be guaranteed the \"unalienable Rights\" of \"Life, Liberty and the pursuit of Happiness.\" It is obvious today that America has defaulted on this promissory note, insofar as her citizens of color are concerned. Instead of honoring this sacred obligation, America has given the Negro people a bad check, a check which has come back marked \"insufficient funds.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "output_string = []\n",
    "for word in words:\n",
    "    if word not in eng_stopwords:\n",
    "        output_string.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'happi', 'join', 'today', 'go', 'histori', 'greatest', 'demonstr', 'freedom', 'histori', 'nation', '.', 'five', 'score', 'year', 'ago', ',', 'great', 'american', ',', 'whose', 'symbol', 'shadow', 'stand', 'today', ',', 'sign', 'emancip', 'proclam', '.', 'thi', 'moment', 'decre', 'came', 'great', 'beacon', 'light', 'hope', 'million', 'negro', 'slave', 'sear', 'flame', 'wither', 'injustic', '.', 'it', 'came', 'joyou', 'daybreak', 'end', 'long', 'night', 'captiv', '.', 'but', 'one', 'hundr', 'year', 'later', ',', 'negro', 'still', 'free', '.', 'one', 'hundr', 'year', 'later', ',', 'life', 'negro', 'still', 'sadli', 'crippl', 'manacl', 'segreg', 'chain', 'discrimin', '.', 'one', 'hundr', 'year', 'later', ',', 'negro', 'live', 'lone', 'island', 'poverti', 'midst', 'vast', 'ocean', 'materi', 'prosper', '.', 'one', 'hundr', 'year', 'later', ',', 'negro', 'still', 'languish', 'corner', 'american', 'societi', 'find', 'exil', 'land', '.', 'and', \"'ve\", 'come', 'today', 'dramat', 'shame', 'condit', '.', 'in', 'sens', \"'ve\", 'come', 'nation', \"'s\", 'capit', 'cash', 'check', '.', 'when', 'architect', 'republ', 'wrote', 'magnific', 'word', 'constitut', 'declar', 'independ', ',', 'sign', 'promissori', 'note', 'everi', 'american', 'fall', 'heir', '.', 'thi', 'note', 'promis', 'men', ',', 'ye', ',', 'black', 'men', 'well', 'white', 'men', ',', 'would', 'guarante', '``', 'unalien', 'right', \"''\", '``', 'life', ',', 'liberti', 'pursuit', 'happi', '.', \"''\", 'it', 'obviou', 'today', 'america', 'default', 'promissori', 'note', ',', 'insofar', 'citizen', 'color', 'concern', '.', 'instead', 'honor', 'sacr', 'oblig', ',', 'america', 'given', 'negro', 'peopl', 'bad', 'check', ',', 'check', 'come', 'back', 'mark', '``', 'insuffici', 'fund', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i happi join today go histori greatest demonstr freedom histori nation . five score year ago , great american , whose symbol shadow stand today , sign emancip proclam . thi moment decre came great beacon light hope million negro slave sear flame wither injustic . it came joyou daybreak end long night captiv . but one hundr year later , negro still free . one hundr year later , life negro still sadli crippl manacl segreg chain discrimin . one hundr year later , negro live lone island poverti midst vast ocean materi prosper . one hundr year later , negro still languish corner american societi find exil land . and 've come today dramat shame condit . in sens 've come nation 's capit cash check . when architect republ wrote magnific word constitut declar independ , sign promissori note everi american fall heir . thi note promis men , ye , black men well white men , would guarante `` unalien right '' `` life , liberti pursuit happi . '' it obviou today america default promissori note , insofar citizen color concern . instead honor sacr oblig , america given negro peopl bad check , check come back mark `` insuffici fund . ''\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
